{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classification - Experimento\n",
    "\n",
    "O Classificador Support-Vector Machine (SVM) é um dos algorítmos de predição mais robustos, e é baseado em métodos de aprendizagem estatística. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png\" alt=\"SVM\" width=\"400\"/>\n",
    "\n",
    "O SVM mapeia dados de treinamento para maximizar o espaço entre as classes de dados a serem separadas. Além de fazer classificações lineares, os SVMs também podem realizar eficientemente predições não-lineares, utilizando abordagens como o [kernel trick](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick) \n",
    "\n",
    "Este componente treina um modelo Support Vector Classification usando [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). <br>\n",
    "\n",
    "Scikit-learn é uma biblioteca open source de machine learning que suporta apredizado supervisionado e não supervisionado. Também provê diversas ferramentas para montagem de modelo, pré-processamento de dados, seleção e avaliação de modelos, e muitos outros utilitários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaração de parâmetros e hiperparâmetros\n",
    "\n",
    "Declare parâmetros com o botão <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAABhWlDQ1BJQ0MgcHJvZmlsZQAAKJF9kT1Iw0AcxV9TtaIVBzuIOASpThb8QhylikWwUNoKrTqYXPohNGlIUlwcBdeCgx+LVQcXZ10dXAVB8APEydFJ0UVK/F9SaBHjwXE/3t173L0DhFqJqWbbGKBqlpGMRcVMdkUMvKID3QhiCOMSM/V4aiENz/F1Dx9f7yI8y/vcn6NHyZkM8InEs0w3LOJ14ulNS+e8TxxiRUkhPiceNeiCxI9cl11+41xwWOCZISOdnCMOEYuFFpZbmBUNlXiKOKyoGuULGZcVzluc1VKFNe7JXxjMacsprtMcRAyLiCMBETIq2EAJFiK0aqSYSNJ+1MM/4PgT5JLJtQFGjnmUoUJy/OB/8LtbMz854SYFo0D7i21/DAOBXaBete3vY9uunwD+Z+BKa/rLNWDmk/RqUwsfAb3bwMV1U5P3gMsdoP9JlwzJkfw0hXweeD+jb8oCfbdA16rbW2Mfpw9AmrpaugEODoGRAmWveby7s7W3f880+vsBocZyukMJsmwAAAAGYktHRAD/AP8A/6C9p5MAAAAJcEhZcwAADdcAAA3XAUIom3gAAAAHdElNRQfkBgsMIwnXL7c0AAACDUlEQVQ4y92UP4gTQRTGf29zJxhJZ2NxbMBKziYWlmJ/ile44Nlkd+dIYWFzItiNgoIEtFaTzF5Ac/inE/urtLWxsMqmUOwCEpt1Zmw2xxKi53XitPO9H9978+aDf/3IUQvSNG0450Yi0jXG7C/eB0cFeu9viciGiDyNoqh2KFBrHSilWstgnU7nFLBTgl+ur6/7PwK11kGe5z3n3Hul1MaiuCgKDZwALHA7z/Oe1jpYCtRaB+PxuA8kQM1aW68Kt7e3zwBp6a5b1ibj8bhfhQYVZwMRiQHrvW9nWfaqCrTWPgRWvPdvsiy7IyLXgEJE4slk8nw+T5nDgDbwE9gyxryuwpRSF5xz+0BhrT07HA4/AyRJchUYASvAbhiGaRVWLIMBYq3tAojIszkMoNRulbXtPM8HwV/sXSQi54HvQRDcO0wfhGGYArvAKjAq2wAgiqJj3vsHpbtur9f7Vi2utLx60LLW2hljEuBJOYu9OI6vAzQajRvAaeBLURSPlsBelA+VhWGYaq3dwaZvbm6+m06noYicE5ErrVbrK3AXqHvvd4bD4Ye5No7jSERGwKr3Pms2m0pr7Rb30DWbTQWYcnFvAieBT7PZbFB1V6vVfpQaU4UtDQetdTCZTC557/eA48BlY8zbRZ1SqrW2tvaxCvtt2iRJ0i9/xb4x5uJRwmNlaaaJ3AfqIvKY/+78Av++6uiSZhYMAAAAAElFTkSuQmCC\" /> na barra de ferramentas.<br>\n",
    "A variável `dataset` possui o caminho para leitura do arquivos importados na tarefa de \"Upload de dados\".<br>\n",
    "Você também pode importar arquivos com o botão <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAABhWlDQ1BJQ0MgcHJvZmlsZQAAKJF9kT1Iw0AcxV9TtaIVBzuIOASpThb8QhylikWwUNoKrTqYXPohNGlIUlwcBdeCgx+LVQcXZ10dXAVB8APEydFJ0UVK/F9SaBHjwXE/3t173L0DhFqJqWbbGKBqlpGMRcVMdkUMvKID3QhiCOMSM/V4aiENz/F1Dx9f7yI8y/vcn6NHyZkM8InEs0w3LOJ14ulNS+e8TxxiRUkhPiceNeiCxI9cl11+41xwWOCZISOdnCMOEYuFFpZbmBUNlXiKOKyoGuULGZcVzluc1VKFNe7JXxjMacsprtMcRAyLiCMBETIq2EAJFiK0aqSYSNJ+1MM/4PgT5JLJtQFGjnmUoUJy/OB/8LtbMz854SYFo0D7i21/DAOBXaBete3vY9uunwD+Z+BKa/rLNWDmk/RqUwsfAb3bwMV1U5P3gMsdoP9JlwzJkfw0hXweeD+jb8oCfbdA16rbW2Mfpw9AmrpaugEODoGRAmWveby7s7W3f880+vsBocZyukMJsmwAAAAGYktHRAD/AP8A/6C9p5MAAAAJcEhZcwAADdcAAA3XAUIom3gAAAAHdElNRQfkBgsOBy6ASTeXAAAC/0lEQVQ4y5WUT2gcdRTHP29m99B23Uiq6dZisgoWCxVJW0oL9dqLfyhCvGWY2YUBI95MsXgwFISirQcLhS5hfgk5CF3wJIhFI7aHNsL2VFZFik1jS1qkiZKdTTKZ3/MyDWuz0fQLc/m99/vMvDfv+4RMlUrlkKqeAAaBAWAP8DSgwJ/AXRG5rao/WWsvTU5O3qKLBMD3fSMiPluXFZEPoyj67PGAMzw83PeEMABHVT/oGpiamnoAmCcEWhH5tFsgF4bh9oWFhfeKxeJ5a+0JVT0oImWgBPQCKfAQuAvcBq67rltX1b+6ApMkKRcKhe9V9QLwbavV+qRer692Sx4ZGSnEcXw0TdP3gSrQswGYz+d/S5IkVtXTwOlCoZAGQXAfmAdagAvsAErtdnuXiDy6+023l7qNRsMODg5+CawBzwB9wFPA7mx8ns/KL2Tl3xCRz5eWlkabzebahrHxPG+v4zgnc7ncufHx8Z+Hhoa29fT0lNM03Q30ikiqqg+ttX/EcTy3WTvWgdVqtddaOw/kgXvADHBHROZVNRaRvKruUNU+EdkPfGWM+WJTYOaSt1T1LPDS/4zLWWPMaLVaPWytrYvIaBRFl/4F9H2/JCKvGmMu+76/X0QOqGoZKDmOs1NV28AicMsYc97zvFdc1/0hG6kEeNsY83UnsCwivwM3VfU7YEZE7lhr74tIK8tbnJiYWPY8b6/ruleAXR0ftQy8boyZXi85CIIICDYpc2ZgYODY3NzcHmvt1eyvP64lETkeRdE1yZyixWLx5U2c8q4x5mIQBE1g33/0d3FlZeXFR06ZttZesNZejuO4q1NE5CPgWVV9E3ij47wB1IDlJEn+ljAM86urq7+KyAtZTgqsO0VV247jnOnv7/9xbGzMViqVMVX9uANYj6LonfVtU6vVkjRNj6jqGeCXzGrPAQeA10TkuKpOz87ONrayhnIA2Qo7BZwKw3B7kiRloKSqO13Xja21C47jPNgysFO1Wi0GmtmzQap6DWgD24A1Vb3SGf8Hfstmz1CuXEIAAAAASUVORK5CYII=\" /> na barra de ferramentas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "dataset = \"/tmp/data/iris.csv\" #@param {type:\"string\"}\n",
    "target = \"Species\" #@param {type:\"feature\", label:\"Atributo alvo\", description: \"Seu modelo será treinado para prever os valores do alvo.\"}\n",
    "use_split = \"Não\" #@param [\"Sim\", \"Não\"] {type:\"feature\",label:\"Usar data-split?\", description: \"Casos tenha usado a componente Data-Split, deseja usar essa divisão como conjunto de treino?\"}\n",
    "\n",
    "# selected features to perform the model\n",
    "filter_type = \"remover\" #@param [\"incluir\",\"remover\"] {type:\"string\",multiple:false,label:\"Modo de seleção das features\",description:\"Se deseja informar quais features deseja incluir no modelo, selecione a opção 'incluir'. Caso deseje informar as features que não devem ser utilizadas, selecione 'remover'. \"}\n",
    "model_features = \"\" #@param {type:\"feature\",multiple:true,label:\"Features para incluir/remover no modelo\",description:\"Seu modelo será feito considerando apenas as features selecionadas. Caso nada seja especificado, todas as features serão utilizadas\"}\n",
    "\n",
    "# features to apply Ordinal Encoder\n",
    "ordinal_features = \"\" #@param {type:\"feature\",multiple:true,label:\"Features para fazer codificação ordinal\", description: \"Seu modelo utilizará a codificação ordinal para as features selecionadas. As demais features categóricas serão codificadas utilizando One-Hot-Encoding.\"}\n",
    "\n",
    "# hyperparameters\n",
    "random_search = \"Sim\" #@param [\"Sim\", \"Não\"] {type:\"string\", label:\"Selecione para mim os melhores parâmetros\", description:\"Será testado uma série de combinações dos parametros abaixo, a melhor combinação será salva na pipeline.\"}\n",
    "kernel = \"rbf\" #@param [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"] {type:\"string\", label:\"Kernel\", description:\"Especifica o tipo de kernel a ser usado no algoritmo\"}\n",
    "C = 1.0 #@param {type:\"number\", label:\"Regularização\", description:\"A força da regularização é inversamente proporcional a C. Deve ser positivo. Penalidade é l2²\"}\n",
    "degree = 3 #@param {type:\"integer\", label:\"Grau\", description:\"Grau da função polinomial do kernel ('poly'). Ignorado por outros kernels\"}\n",
    "gamma = \"auto\" #@param [\"scale\", \"auto\"] {type: \"string\", label:\"Gama\", description:\"Coeficiente de kernel para 'rbf', 'poly' e 'sigmoid'\"} \n",
    "max_iter = -1 #@param {type:\"integer\", label:\"Iteração\", description:\"Limite fixo nas iterações no solver, ou -1 sem limite\"}\n",
    "probability = True #@param {type: \"boolean\", label:\"Probabilidade\", description:\"Se é necessário ativar estimativas de probabilidade.  Deve ser ativado antes da chamada fit() do modelo, reduzirá a velocidade desse método, pois ele usa internamente o 5-fold-cross-validation, e predict_proba pode ser inconsistente com a chamada predict()\"}\n",
    "\n",
    "# predict method\n",
    "method = \"predict_proba\" #@param [\"predict_proba\", \"predict\"] {type:\"string\", label:\"Método de Predição\", description:\"Se optar por 'predict_proba', o método de predição será a probabilidade estimada de cada classe, já o 'predict' prediz a qual classe pertence\"} \n",
    "\n",
    "# Plots to ignore\n",
    "plots_ignore = [\"\"] #@param [\"Dados de Teste\", \"Matriz de Confusão\", \"Métricas Comuns\", \"Curva ROC\", \"Tabelas de Dados\", \"SHAP\"] {type:\"string\",multiple:true,label:\"Gráficos a serem ignorados\", description: \"Diversos gráficos são gerados ao executar o treinamento e validação do modelo, selecione quais não devem ser gerados.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acesso ao conjunto de dados\n",
    "\n",
    "O conjunto de dados utilizado nesta etapa será o mesmo carregado através da plataforma.<br>\n",
    "O tipo da variável retornada depende do arquivo de origem:\n",
    "- [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) para CSV e compressed CSV: .csv .csv.zip .csv.gz .csv.bz2 .csv.xz\n",
    "- [Binary IO stream](https://docs.python.org/3/library/io.html#binary-i-o) para outros tipos de arquivo: .jpg .wav .zip .h5 .parquet etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acesso aos metadados do conjunto de dados\n",
    "\n",
    "Utiliza a função `stat_dataset` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para carregar metadados. <br>\n",
    "Por exemplo, arquivos CSV possuem `metadata['featuretypes']` para cada coluna no conjunto de dados (ex: categorical, numerical, or datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from platiagro import stat_dataset\n",
    "\n",
    "metadata = stat_dataset(name=dataset)\n",
    "featuretypes = metadata[\"featuretypes\"]\n",
    "\n",
    "columns = df.columns.to_numpy()\n",
    "featuretypes = np.array(featuretypes)\n",
    "target_index = np.argwhere(columns == target)\n",
    "featuretypes = np.delete(featuretypes, target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de linhas com valores faltantes no atributo alvo\n",
    "\n",
    "Caso haja linhas em que o atributo alvo contenha valores faltantes, é feita a remoção dos casos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[target], inplace=True)\n",
    "y = df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem das features \n",
    "\n",
    "Seleciona apenas as features que foram declaradas no parâmetro model_features. Se nenhuma feature for especificada, todo o conjunto de dados será utilizado para a modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_filter = columns\n",
    "\n",
    "if len(model_features) >= 1:\n",
    "\n",
    "    if filter_type == \"incluir\":\n",
    "        columns_index = (np.where(np.isin(columns, model_features)))[0]\n",
    "        columns_index.sort()\n",
    "        columns_to_filter = columns[columns_index]\n",
    "        featuretypes = featuretypes[columns_index]\n",
    "    else:\n",
    "        columns_index = (np.where(np.isin(columns, model_features)))[0]\n",
    "        columns_index.sort()\n",
    "        columns_to_filter = np.delete(columns, columns_index)\n",
    "        featuretypes = np.delete(featuretypes, columns_index)\n",
    "\n",
    "# keep the features selected\n",
    "df_model = df[columns_to_filter]\n",
    "\n",
    "columns_to_filter = np.delete(columns_to_filter, target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset em subconjuntos de treino e teste\n",
    "\n",
    "Carregando artefatos do componente Data-Split, se esse foi ultilizado antes, e separar treino e teste. <br>\n",
    "Subconjunto de treino: amostra de dados usada para treinar o modelo.<br>\n",
    "Subconjunto de teste: amostra de dados usada para fornecer uma avaliação imparcial do treinamento do modelo no subconjunto de dados de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os.path\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "file_path = \"data-split1.joblib\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    artifacts = joblib.load(file_path)\n",
    "\n",
    "    if artifacts[\"use_kfolds\"] != None:\n",
    "        cross_val = KFold(n_splits=use_kfolds, shuffle=True, random_state=42)     \n",
    "    elif artifacts[\"use_loo\"] != None:   \n",
    "        cross_val = LeaveOneOut()\n",
    "    else:\n",
    "    # caso eu não esteja usando split mas talvez venha a usar o RandomizedSearchCV\n",
    "        cross_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# verificando split para separar treino/teste\n",
    "if use_split == \"Sim\" and \"data-split\" in df_model.columns:\n",
    "    if use_kfolds != None or use_loo != None:\n",
    "        # quando uso split e uso cv\n",
    "        X_test =  df_model[df_model['data-split']==0].drop([target, \"data-split\"], axis=1).to_numpy()\n",
    "        X_train =  df_model[df_model['data-split']==1].drop([target, \"data-split\"], axis=1).to_numpy()\n",
    "        y_test = df_model[df_model['data-split']==0][target].to_numpy()\n",
    "        y_train = df_model[df_model['data-split']==1][target].to_numpy()\n",
    "\n",
    "    else:\n",
    "        # quando uso split e não uso cv\n",
    "        X_test =  df_model[df_model['data-split']==2].drop([target, \"data-split\"], axis=1).to_numpy()\n",
    "        X_train =  df_model[df_model['data-split']==1].drop([target, \"data-split\"], axis=1).to_numpy()\n",
    "        y_test = df_model[df_model['data-split']==2][target].to_numpy()\n",
    "        y_train = df_model[df_model['data-split']==1][target].to_numpy() \n",
    "    # removendo a coluna \"data-split\" dos metadados    \n",
    "    featuretypes = featuretypes[:-1]\n",
    "    columns_to_filter = columns_to_filter[:-1]\n",
    "\n",
    "else:\n",
    "    # quando não quero usar o split\n",
    "    # verificar se tenho a coluna\n",
    "    if \"data-split\" in df_model.columns:\n",
    "        featuretypes = featuretypes[:-1]\n",
    "        columns_to_filter = columns_to_filter[:-1]\n",
    "        X = df_model.drop([target, \"data-split\"], axis=1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)    \n",
    "    else:\n",
    "        # não preciso alterar\n",
    "        X = df_model.drop([target], axis=1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codifica labels do atributo alvo\n",
    "\n",
    "As labels do atributo alvo são convertidos em números inteiros ordinais com valor entre 0 e n_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração dos atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.featuretypes import NUMERICAL\n",
    "\n",
    "# Selects the indexes of numerical and non-numerical features\n",
    "numerical_indexes = np.where(featuretypes == NUMERICAL)[0]\n",
    "non_numerical_indexes = np.where(~(featuretypes == NUMERICAL))[0]\n",
    "\n",
    "# Selects non-numerical features to apply ordinal encoder or one-hot encoder\n",
    "ordinal_features = np.array(ordinal_features)\n",
    "\n",
    "non_numerical_indexes_ordinal = np.where(\n",
    "    ~(featuretypes == NUMERICAL) & np.isin(columns_to_filter, ordinal_features)\n",
    ")[0]\n",
    "\n",
    "non_numerical_indexes_one_hot = np.where(\n",
    "    ~(featuretypes == NUMERICAL) & ~(np.isin(columns_to_filter, ordinal_features))\n",
    ")[0]\n",
    "\n",
    "# After the step handle_missing_values,\n",
    "# numerical features are grouped in the beggining of the array\n",
    "numerical_indexes_after_handle_missing_values = np.arange(len(numerical_indexes))\n",
    "\n",
    "non_numerical_indexes_after_handle_missing_values = np.arange(\n",
    "    len(numerical_indexes), len(featuretypes)\n",
    ")\n",
    "\n",
    "ordinal_indexes_after_handle_missing_values = non_numerical_indexes_after_handle_missing_values[\n",
    "    np.where(np.isin(non_numerical_indexes, non_numerical_indexes_ordinal))[0]\n",
    "]\n",
    "\n",
    "one_hot_indexes_after_handle_missing_values = non_numerical_indexes_after_handle_missing_values[\n",
    "    np.where(np.isin(non_numerical_indexes, non_numerical_indexes_one_hot))[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treina um modelo usando sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"handle_missing_values\",\n",
    "            ColumnTransformer(\n",
    "                [\n",
    "                    (\"imputer_mean\", SimpleImputer(strategy=\"mean\"), numerical_indexes),\n",
    "                    (\n",
    "                        \"imputer_mode\",\n",
    "                        SimpleImputer(strategy=\"most_frequent\"),\n",
    "                        non_numerical_indexes,\n",
    "                    ),\n",
    "                ],\n",
    "                remainder=\"drop\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"handle_categorical_features\",\n",
    "            ColumnTransformer(\n",
    "                [\n",
    "                    (\n",
    "                        \"feature_encoder_ordinal\",\n",
    "                        OrdinalEncoder(),\n",
    "                        ordinal_indexes_after_handle_missing_values,\n",
    "                    ),\n",
    "                    (\n",
    "                        \"feature_encoder_onehot\",\n",
    "                        OneHotEncoder(),\n",
    "                        one_hot_indexes_after_handle_missing_values,\n",
    "                    ),\n",
    "                ],\n",
    "                remainder=\"passthrough\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"estimator\",\n",
    "            SVC(\n",
    "                C=C,\n",
    "                kernel=kernel,\n",
    "                degree=degree,\n",
    "                gamma=gamma,\n",
    "                probability=probability,\n",
    "                max_iter=max_iter,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "features_after_pipeline = np.concatenate(\n",
    "    (columns_to_filter[numerical_indexes], columns_to_filter[non_numerical_indexes])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecionando os parâmetros\n",
    "\n",
    "Ultilizando RandomizedSearchCV da biblioteca Scikit-learn para encontrar a melhor combinação de hiperparâmetros para esse modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    \n",
    "if random_search != \"Sim\":\n",
    "    \n",
    "    param_grid = { \n",
    "        'estimator__C': [0, 1.0, 1.25, 1,5],\n",
    "        'estimator__kernel': [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"],\n",
    "        'estimator__degree': [2, 3, 4, 5],\n",
    "        'estimator__gamma': [\"scale\", \"auto\"] ,\n",
    "        'estimator__class_weight': [None, \"balanced\", \"balanced_subsample\"],\n",
    "        'estimator__bootstrap': [True, False],\n",
    "    }\n",
    "    \n",
    "    def_search = RandomizedSearchCV(pipeline, param_grid, random_state=42, cv = cross_val)\n",
    "    search = def_search.fit(X_train, y_train)\n",
    "    best_par = search.best_params_\n",
    "\n",
    "    #atualizando parametros\n",
    "    pipeline.set_params(estimator__C=best_par[\"estimator__C\"],\n",
    "                        estimator__kernel = best_par[\"estimator__kernel\"],\n",
    "                        estimator__degree = best_par[\"estimator__degree\"],\n",
    "                        estimator__gamma = best_par[\"estimator__gamma\"],\n",
    "                        estimator__class_weight = best_par[\"estimator__class_weight\"],\n",
    "                        estimator__bootstrap = best_par[\"estimator__bootstrap\"]\n",
    "                       )\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "else: \n",
    "    \n",
    "    if cross_val != None:\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = cross_val_predict(pipeline, X_test, y_test, cv = cross_val)\n",
    "\n",
    "    else:    \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização de desempenho\n",
    "\n",
    "A [**Matriz de Confusão**](https://en.wikipedia.org/wiki/Confusion_matrix) (Confusion Matrix) é uma tabela que nos permite a visualização do desempenho de um algoritmo de classificação. <br>\n",
    "É extremamente útil para mensurar [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification), [Recall, Precision, and F-measure](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# uses the model to make predictions on the Test Dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)\n",
    "\n",
    "# computes confusion matrix\n",
    "labels = np.unique(y)\n",
    "data = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "# puts matrix in pandas.DataFrame for better format\n",
    "labels_dec = label_encoder.inverse_transform(labels)\n",
    "confusion_matrix = pd.DataFrame(data, columns=labels_dec, index=labels_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva métricas\n",
    "\n",
    "Utiliza a função `save_metrics` do [SDK da PlatIAgro](https://platiagro.github.io/sdk/) para salvar métricas. Por exemplo: `accuracy`, `precision`, `r2_score`, `custom_score` etc.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_metrics\n",
    "\n",
    "save_metrics(confusion_matrix=confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiza resultados\n",
    "A avaliação do desempenho do modelo pode ser feita por meio da análise da Curva ROC (ROC). Esse gráfico permite avaliar a performance de um classificador binário para diferentes pontos de cortes. A métrica AUC (Area under curve) também é calculada e indicada na legenda do gráfico. Se a variável resposta tiver mais de duas categorias, o cálculo da curva ROC e AUC é feito utilizando o algoritmo one-vs-rest, ou seja, calcula-se a curva ROC e AUC de cada classe em relação ao restante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.plotting import plot_classification_data\n",
    "\n",
    "if \"Dados de Teste\" not in plots_ignore:\n",
    "    \n",
    "    plot_classification_data(pipeline, columns_to_filter, X_train, y_train, X_test, y_test, y_pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.plotting import plot_matrix\n",
    "\n",
    "if \"Matriz de Confusão\" not in plots_ignore:\n",
    "    \n",
    "    plot_matrix(confusion_matrix)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.plotting import plot_common_metrics\n",
    "\n",
    "if \"Métricas Comuns\" not in plots_ignore:\n",
    "    \n",
    "    plot_common_metrics(y_test, y_pred, labels, labels_dec)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.plotting import plot_roc_curve\n",
    "\n",
    "if \"Curva ROC\" not in plots_ignore:\n",
    "    \n",
    "    plot_roc_curve(y_test, y_prob, labels_dec)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por meio da bibiloteca SHAP retornamos o gráfico abaixo, através do qual é possível visualziar o impacto das features de entrada para cada possível classe na saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.plotting import plot_shap_classification_summary\n",
    "\n",
    "if \"SHAP\" not in plots_ignore:\n",
    "\n",
    "    plot_shap_classification_summary(pipeline, X_train,y_train,columns_to_filter,label_encoder,non_numerical_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização da tabela contendo os resultados finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.plotting import plot_data_table\n",
    "\n",
    "if \"Tabelas de Dados\" not in plots_ignore:\n",
    "\n",
    "    df_test = pd.DataFrame(X_test, columns=columns_to_filter)\n",
    "    labels_prob = list(label_encoder.inverse_transform(np.unique(y)))\n",
    "    labels_prob = [str(label) +'_prob' for label in labels_prob]\n",
    "    new_cols = [target] + labels_prob\n",
    "\n",
    "    y_pred_expanded = np.expand_dims(label_encoder.inverse_transform(y_pred), axis=1)\n",
    "    y_arrays = np.concatenate((y_pred_expanded, y_prob), axis=1)\n",
    "    for col, y_array  in zip(new_cols,y_arrays.T):\n",
    "        df_test[col] = y_array\n",
    "\n",
    "    ax = plot_data_table(df_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva alterações no conjunto de dados\n",
    "\n",
    "O conjunto de dados será salvo (e sobrescrito com as respectivas mudanças) localmente, no container da experimentação, utilizando a função `pandas.DataFrame.to_csv`.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "def generate_col_names(labels_dec):\n",
    "    return [\n",
    "        sub(\"[^a-zA-Z0-9\\n\\.]\", \"_\", str(\"SVC_predict_proba\" + \"_\" + str(class_i)))\n",
    "        for i, class_i in enumerate(labels_dec)\n",
    "    ] \n",
    "\n",
    "pipeline.fit(X, y)\n",
    "y_prob = pipeline.predict_proba(X)\n",
    "y_pred = pipeline.predict(X)\n",
    "\n",
    "new_columns = generate_col_names(labels_dec)\n",
    "df.loc[:, new_columns] = y_prob\n",
    "\n",
    "y_pred = label_encoder.inverse_transform(y_pred)\n",
    "df.loc[:, \"SVC_predict_class\"] = y_pred\n",
    "new_columns = new_columns + [\"SVC_predict_class\"]\n",
    "\n",
    "# save dataset changes\n",
    "df.to_csv(dataset, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva resultados da tarefa \n",
    "\n",
    "A plataforma guarda o conteúdo de `/tmp/data/` para as tarefas subsequentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "artifacts = {\n",
    "    \"pipeline\": pipeline,\n",
    "    \"label_encoder\": label_encoder,\n",
    "    \"columns\": columns,\n",
    "    \"columns_to_filter\": columns_to_filter,\n",
    "    \"new_columns\": new_columns,\n",
    "    \"method\": method,\n",
    "    \"features_after_pipeline\": features_after_pipeline,\n",
    "}\n",
    "\n",
    "dump(artifacts, \"/tmp/data/svc.joblib\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "experiment_id": "adbf4d9e-900f-4221-a499-f2a10be05614",
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "operator_id": "eb9267f9-9724-4076-b4e6-50d98affeb84",
  "task_id": "b0d080a8-cede-4bdc-879a-50d6368962c4"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
